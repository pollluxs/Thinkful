{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from sklearn import preprocessing\n",
    "import math\n",
    "from matplotlib.mlab import PCA as mlabPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab and process the raw data.\n",
    "data_path = (\"https://raw.githubusercontent.com/Thinkful-Ed/data-201-resources/\"\n",
    "             \"master/sms_spam_collection/SMSSpamCollection\"\n",
    "            )\n",
    "sms_raw = pd.read_csv(data_path, delimiter= '\\t', header=None)\n",
    "sms_raw.columns = ['spam', 'message']\n",
    "\n",
    "# Enumerate our spammy keywords.\n",
    "keywords = ['click', 'offer', 'winner', 'buy', 'free', 'cash', 'urgent']\n",
    "\n",
    "for key in keywords:\n",
    "    sms_raw[str(key)] = sms_raw.message.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=False\n",
    ")\n",
    "\n",
    "sms_raw['allcaps'] = sms_raw.message.str.isupper()\n",
    "sms_raw['spam'] = (sms_raw['spam'] == 'spam')\n",
    "data = sms_raw[keywords + ['allcaps']]\n",
    "target = sms_raw['spam']\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB()\n",
    "y_pred = bnb.fit(data, target).predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>message</th>\n",
       "      <th>click</th>\n",
       "      <th>offer</th>\n",
       "      <th>winner</th>\n",
       "      <th>buy</th>\n",
       "      <th>free</th>\n",
       "      <th>cash</th>\n",
       "      <th>urgent</th>\n",
       "      <th>allcaps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    spam                                            message  click  offer  \\\n",
       "0  False  Go until jurong point, crazy.. Available only ...  False  False   \n",
       "1  False                      Ok lar... Joking wif u oni...  False  False   \n",
       "2   True  Free entry in 2 a wkly comp to win FA Cup fina...  False  False   \n",
       "3  False  U dun say so early hor... U c already then say...  False  False   \n",
       "4  False  Nah I don't think he goes to usf, he lives aro...  False  False   \n",
       "\n",
       "   winner    buy   free   cash  urgent  allcaps  \n",
       "0   False  False  False  False   False    False  \n",
       "1   False  False  False  False   False    False  \n",
       "2   False  False  False  False   False    False  \n",
       "3   False  False  False  False   False    False  \n",
       "4   False  False  False  False   False    False  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = sms_raw.shape[0]\n",
    "correctly_classified = (target == y_pred).sum()\n",
    "incorrectly_classified = (target != y_pred).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Correct:  0.89160086145\n",
      "% Incorrect:  0.10839913855\n"
     ]
    }
   ],
   "source": [
    "print('% Correct: ', correctly_classified/data_size)\n",
    "print('% Incorrect: ', incorrectly_classified/data_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4770,   55],\n",
       "       [ 549,  198]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(target, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the columns are prediction and the rows are actual.\n",
    "\n",
    "So what do we learn?\n",
    "\n",
    "We learn the majority of our error is coming from times where we failed to identify a spam message. 549 of our 604 errors are from failing to identify spam. So we need to get a little bit better at identifying spam messages.\n",
    "\n",
    "But before we move on or iterate on the model, let's talk about some key terms that you may run into when thinking about this kind of matrix.\n",
    "\n",
    "Let's assume our goal is to identify spam (rather than identify ham).\n",
    "\n",
    "Firstly, when we talk about errors in a binary classifier (where there are only two outcomes) we're generally referring to two kinds of errors. A false positive is when we identify something as spam that is not. In this case we had 55 of these. This is sometimes also called a \"Type I Error\" or a \"false alarm\".\n",
    "\n",
    "A false negative is therefore when we mistakenly identify something as not spam when it is. We had 549 of these. This is also called a \"Type II Error\" or a \"miss\".\n",
    "\n",
    "This also brings us to a conversation of sensitivity vs specificity.\n",
    "\n",
    "Sensitivity is the percentage of positives correctly identified, in our case 198/747 or 27%. This shows how good we are at catching positives, or how sensitive our model is to identifying positives.\n",
    "\n",
    "Specificity is just the opposite, the percentage of negatives correctly identified, 4770/4825 or 99%.\n",
    "\n",
    "Again this confirms that we're not great at identifying spam, though we do label ham quite accurately. You should get familiar with these terms as in the practicing world they will often be used with little explanation and you will be expected to understand them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drill:\n",
    "\n",
    "It's worth calculating these with code so that you fully understand how these statistics work, so here is your task for the cell below. Manually generate (meaning don't use the SKLearn function) your own confusion matrix and print it along with the sensitivity and specificity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = sms_raw.shape[0]\n",
    "correctly_classified = ((target == y_pred)&(target==True)).sum()\n",
    "incorrectly_classified = ((target != y_pred)&(target==False)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True], dtype=bool)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_1error = ((target != y_pred)&(target==False)).sum()\n",
    "type_2error = ((target != y_pred)&(target==True)).sum()\n",
    "sensitivity = ((target == y_pred)&(target==True)).sum()\n",
    "specificity = ((target == y_pred)&(target==False)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_1error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "549"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_2error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4770"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4770 55\n",
      "549 198\n"
     ]
    }
   ],
   "source": [
    "# how made confusion matrix\n",
    "print(specificity,type_1error)\n",
    "print(type_2error,sensitivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In Sample evaluation and cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 20% Holdout: 0.884304932735\n",
      "Testing on Sample: 0.89160086145\n"
     ]
    }
   ],
   "source": [
    "# Use train_test_split to create the necessary training and test groups\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=20)\n",
    "print('With 20% Holdout: ' + str(bnb.fit(X_train, y_train).score(X_test, y_test)))\n",
    "print('Testing on Sample: ' + str(bnb.fit(data, target).score(data, target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.89784946,  0.89426523,  0.89426523,  0.890681  ,  0.89605735,\n",
       "        0.89048474,  0.88150808,  0.89028777,  0.88489209,  0.89568345])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(bnb, data, target, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge: Iterate and evaluate your classifier\n",
    "\n",
    "It's time to revisit your classifier from the previous assignment. Using the evaluation techniques we've covered here, look at your classifier's performance in more detail. Then go back and iterate by engineering new features, removing poor features, or tuning parameters. Repeat this process until you have five different versions of your classifier. Once you've iterated, answer these questions to compare the performance of each:\n",
    "\n",
    "Do any of your classifiers seem to overfit?\n",
    "\n",
    "Which seem to perform the best? Why?\n",
    "\n",
    "Which features seemed to be most impactful to performance?\n",
    "\n",
    "Write up your iterations and answers to the above questions in a few pages. Submit a link below and go over it with your mentor to see if they have any other ideas on how you could improve your classifier's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_path = \"https://raw.githubusercontent.com/Thinkful-Ed/data-201-resources/master/sentiment_labelled_sentences/amazon_cells_labelled.txt\"\n",
    "imdb_path =\"https://raw.githubusercontent.com/Thinkful-Ed/data-201-resources/master/sentiment_labelled_sentences/imdb_labelled.txt\"\n",
    "yelp_path =\"https://raw.githubusercontent.com/Thinkful-Ed/data-201-resources/master/sentiment_labelled_sentences/yelp_labelled.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['good', 'great', 'best', 'delicious', 'oustanding', 'bad', 'terrible', 'worst', 'never', 'broken', 'boring', 'deplorable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon = pd.read_csv(amazon_path, delimiter= '\\t', header=None)\n",
    "amazon.columns = [\"Message\",\"Sentiment\"]\n",
    "for key in keywords:\n",
    "    amazon[str(key)] = amazon.Message.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_amazon = amazon[keywords]\n",
    "target_amazon = amazon['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 1000 points : 429\n"
     ]
    }
   ],
   "source": [
    "bnb = BernoulliNB()\n",
    "\n",
    "bnb.fit(data_amazon, target_amazon)\n",
    "\n",
    "# Classify, storing the result in a new variable.\n",
    "y_pred = bnb.predict(data_amazon)\n",
    "\n",
    "# Display our results.\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(data_amazon.shape[0],(target_amazon != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[487,  13],\n",
       "       [416,  84]], dtype=int64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusions matrix for amazon\n",
    "confusion_matrix(target_amazon, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 20% Holdout: 0.575\n",
      "Testing on Sample: 0.571\n"
     ]
    }
   ],
   "source": [
    "X_train_amazon, X_test_amazon, y_train_amazon, y_test_amazon = train_test_split(data_amazon, target_amazon, test_size=0.2, random_state=20)\n",
    "print('With 20% Holdout: ' + str(bnb.fit(X_train_amazon, y_train_amazon).score(X_test_amazon, y_test_amazon)))\n",
    "print('Testing on Sample: ' + str(bnb.fit(data_amazon, target_amazon).score(data_amazon, target_amazon)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model doesn't seem to be overfitting, just not very good either"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.56,  0.59,  0.59,  0.57,  0.57,  0.58,  0.56,  0.57,  0.59,  0.53])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(bnb, data_amazon, target_amazon, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supports that the model isn't overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good',\n",
       " 'great',\n",
       " 'best',\n",
       " 'delicious',\n",
       " 'oustanding',\n",
       " 'bad',\n",
       " 'terrible',\n",
       " 'worst',\n",
       " 'never',\n",
       " 'broken',\n",
       " 'boring',\n",
       " 'deplorable']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Five different keyword combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_1 = ['good', 'great', 'best', 'oustanding', 'bad', 'terrible', 'worst', 'never', 'broken', 'boring']\n",
    "keywords_2 = ['good', 'great', 'best', 'oustanding', 'amazing']\n",
    "keywords_3 = ['bad', 'terrible', 'worst', 'never', 'broken', 'boring', 'deplorable']\n",
    "keywords_4 = ['good','bad']\n",
    "keywords_5 = ['best', 'worst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_lists = [keywords_1, keywords_2, keywords_3, keywords_4, keywords_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_analyzer(data_path, groups):\n",
    "    data_input = pd.read_csv(data_path, delimiter= '\\t', header=None)\n",
    "    data_input.columns = ['Message','Sentiment']\n",
    "    for group in groups:\n",
    "        for word in group:\n",
    "            data_input[str(word)] = data_input.Message.str.contains(' ' + str(word) + ' ',case=False)\n",
    "        data = data_input[group]\n",
    "        target = data_input['Sentiment']\n",
    "        bnb = BernoulliNB()\n",
    "        bnb.fit(data, target)\n",
    "        y_pred = bnb.predict(data)\n",
    "        print(\"Number of mislabeled points out of a total {} points : {}\".format(data.shape[0],(target != y_pred).sum()))\n",
    "        print(\"Confusion Matrix: \",\"\\n\", confusion_matrix(target, y_pred))\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=20)\n",
    "        print('With 20% Holdout: ' + str(bnb.fit(X_train, y_train).score(X_test, y_test)))\n",
    "        print('Testing on Sample: ' + str(bnb.fit(data, target).score(data, target)))\n",
    "        print(\"Cross Value Scores: \", cross_val_score(bnb, data, target, cv=10))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 1000 points : 429\n",
      "Confusion Matrix:  \n",
      " [[487  13]\n",
      " [416  84]]\n",
      "With 20% Holdout: 0.575\n",
      "Testing on Sample: 0.571\n",
      "Cross Value Scores:  [ 0.56  0.59  0.59  0.57  0.57  0.58  0.56  0.57  0.59  0.53]\n",
      "Number of mislabeled points out of a total 1000 points : 429\n",
      "Confusion Matrix:  \n",
      " [[487  13]\n",
      " [416  84]]\n",
      "With 20% Holdout: 0.575\n",
      "Testing on Sample: 0.571\n",
      "Cross Value Scores:  [ 0.56  0.59  0.59  0.57  0.57  0.58  0.56  0.57  0.59  0.53]\n",
      "Number of mislabeled points out of a total 1000 points : 480\n",
      "Confusion Matrix:  \n",
      " [[ 22 478]\n",
      " [  2 498]]\n",
      "With 20% Holdout: 0.53\n",
      "Testing on Sample: 0.52\n",
      "Cross Value Scores:  [ 0.48  0.5   0.51  0.56  0.52  0.52  0.55  0.5   0.53  0.52]\n",
      "Number of mislabeled points out of a total 1000 points : 474\n",
      "Confusion Matrix:  \n",
      " [[488  12]\n",
      " [462  38]]\n",
      "With 20% Holdout: 0.515\n",
      "Testing on Sample: 0.526\n",
      "Cross Value Scores:  [ 0.5   0.55  0.54  0.54  0.51  0.56  0.51  0.49  0.55  0.51]\n",
      "Number of mislabeled points out of a total 1000 points : 485\n",
      "Confusion Matrix:  \n",
      " [[500   0]\n",
      " [485  15]]\n",
      "With 20% Holdout: 0.5\n",
      "Testing on Sample: 0.515\n",
      "Cross Value Scores:  [ 0.52  0.51  0.52  0.51  0.52  0.5   0.5   0.54  0.52  0.51]\n"
     ]
    }
   ],
   "source": [
    "keyword_analyzer(amazon_path, keyword_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'great', 'best', 'oustanding', 'bad', 'terrible', 'worst', 'never', 'broken', 'boring']\n",
      "['good', 'great', 'best', 'oustanding', 'amazing']\n",
      "['bad', 'terrible', 'worst', 'never', 'broken', 'boring', 'deplorable']\n",
      "['good', 'bad']\n",
      "['best', 'worst']\n"
     ]
    }
   ],
   "source": [
    "for i in keyword_lists:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None seem to overfit, they just aren't particularly good. \n",
    "\n",
    "The ones with the positive words seemed to do the best.\n",
    "\n",
    "The positive word features appear to have the most impact as the number of mislabels went up when they were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
